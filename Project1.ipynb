{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Project 1\n",
      "### Discover as many as possible BitBucket code repositories\n",
      "\n",
      "### Context\n",
      "You work for a cloud application provider DeveloperParadise that needs to convince its investors \n",
      "that their business has a bright future.\n",
      "\n",
      "Your task is to show that the number of customers (users of DeveloperParadise, which happens to \n",
      "compete with Atlassian's BitBucket) is comparable to that of  BitBucket. The presentation for investors \n",
      "is next week (completely incidentally, exactly on the day this assignment is due). \n",
      "\n",
      "Feel free to use examples below or any other information you can gather to get an estimate of the \n",
      "users and repositories hosted on BitBucket.\n",
      "\n",
      "While a template for the search strategy is provided, feel free to use any means (including borrowing with attribution from the other teams) to get as accurate result as possible within this very short but, unfortunately, very realistic time frame.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Example 1. Search for a keyword 'test'\n",
      "\n",
      "Unfortunately, BitBucket does not have a documented search API\n",
      "\n",
      "Feel free to experiment with an undocumented search API below that appears to \n",
      "yeld some results, though it is not clear how complete they are and \n",
      "how exactly the results are provided by BitBucket.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests, json, sys, time, re, BeautifulSoup\n",
      "key = 'test'\n",
      "URL = 'https://bitbucket.org/repo/all?name=' + key\n",
      "r = requests .get(URL)\n",
      "t = r.text\n",
      "soup = BeautifulSoup.BeautifulSoup(t)\n",
      "start = 0\n",
      "npages = 1\n",
      "for link in soup.findAll('a'):\n",
      "    if (start == 3): continue\n",
      "    s = link.get('href')\n",
      "    if (start == 0):\n",
      "        if ('/account/signin/?next' in s):\n",
      "            start = 1\n",
      "            continue\n",
      "        else:\n",
      "            continue   \n",
      "    if  s == '#': continue \n",
      "        \n",
      "    # This is the way remaining pages in the search results are marked\n",
      "    m = re.search(r'^/repo/all/(\\d*)\\?', s)\n",
      "    if  m:\n",
      "        start = 2\n",
      "        #get max num\n",
      "        i = m.group(1)\n",
      "        if (int(i) > npages): npages = int(i)\n",
      "        #print npages\n",
      "    else:\n",
      "        if start == 2: \n",
      "            start = 3\n",
      "            continue\n",
      "        # Print the repo urls\n",
      "        print s\n",
      "        \n",
      "#now need to go over the remaining pages        \n",
      "# from 2 to npages\n",
      "#np = unicode(npages)\n",
      "#print '/repo/all/'+ np + '?name=' + key"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": []
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Task 1A: Retrieve and store links from all pages"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Task 1B: Try several keywords, what keywords yield the largest number of repositories?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Example 2. Search for 'site:bitbucket.org' via google."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests, json, sys, time, re, BeautifulSoup\n",
      "URL = 'https://www.google.com/search?q=site%3Abitbucket.org'\n",
      "r = requests .get(URL)\n",
      "t = r.text\n",
      "soup = BeautifulSoup.BeautifulSoup(t)\n",
      "start = 0\n",
      "npages = 1\n",
      "for link in soup.findAll('a'):\n",
      "    s = link.get('href')\n",
      "    m = re.search(r'url?q=https://bitbucket.org/([^&%]*)', s)\n",
      "    if m:\n",
      "        print m.group[1]    \n",
      "#now need to go over the remaining pages \n",
      "# https://www.google.com/search?q=site%3Abitbucket.org&start=900\n",
      "# until "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": []
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Task 2A: Retrieve repository links over all search pages"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}